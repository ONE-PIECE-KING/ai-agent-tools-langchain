{
  "document_type": "結構化數據",
  "splitter": "TokenTextSplitter",
  "metrics": {
    "chunk_count": 2,
    "avg_chunk_size": 136.5,
    "max_chunk_size": 219,
    "min_chunk_size": 54,
    "processing_time": 0.0004640999832190573,
    "overlap_ratio": 0.0
  },
  "chunks": [
    {
      "index": 0,
      "content": "Model,Accuracy,Training Time,Memory Usage,Description\nBERT-base,0.92,12.5,5.2,\"基礎BERT模型，適合一般NLP任務\"\nRoBERTa-large,0.95,24.3,10.8,\"改進版BERT，性能更優但資源消耗較大\"\nGPT-3,0.98,168.0,45.6,\"大型語言模型，功能強大但成本高\"\nT5-small,0.89,8.2,3.4,\"輕量級T5�",
      "length": 219
    },
    {
      "index": 1,
      "content": "��量級T5模型，適合資源受限場景\"\nBART,0.93,15.7,7.1,\"序列到序列模型，適合生成任務\"",
      "length": 54
    }
  ]
}